# Approaches to Fairness and Bias Mitigation in Natural Language processing

(presenter speaks quickly and topic is complex - hard to take notes, recc watching the talk when it's uploaded to YouTube)

Describes various harms

Fairness meaures

A fairness evaluation can be:
- Disaggregated - eg only men or women
- intersectional - eg asian woman or american men

Fairnes Metrics

- group fairness
- individual fairness

Group Fairness Measures
- equality of opportunity - eg the % of qualified students who are admitted are the same percentage
- equalized odds - eg the % of unqualified students who are not admitted are same percentage
- demographic parity - you are looking at percentage as a whole

Techniques for Bias Detection

We can look for biases both in represensations and in model predictions

IAT - Implicit Association Test

WEAT - word embeddign association test - eg when you hear engineer - do you think of man/male

